# -*- coding: utf-8 -*-
"""Chatbot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13yvuLxzAH3wA_oKXJMX4HbqMk8AiVioQ

**Pre-trained model for generating response**
"""

# !pip install -q -U google-generativeai

import pathlib
import textwrap

import google.generativeai as genai

from IPython.display import display
from IPython.display import Markdown

from google.colab import userdata


import os
os.environ['GOOGLE_API_KEY']="GOOGLE_API_KEY"
genai.configure(api_key="GOOGLE_API_KEY")

model = genai.GenerativeModel('gemini-pro')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("hello gemini?")

response.text

"""**PDF** **parsing**

"""

!pip install PyMuPDF nltk

import nltk
nltk.download('punkt')

import fitz  # PyMuPDF
import nltk

# Ensure the punkt tokenizer is available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

def extract_sentences_from_pdf(pdf_path):
    # Open the PDF file
    with fitz.open(pdf_path) as doc:
        text = ""
        for page in doc:
            text += page.get_text()

    # Tokenize the text into sentences
    sentences = nltk.tokenize.sent_tokenize(text)
    return sentences

#pdf path
pdf_path = "/content/Mark_Zuckerberg_Biography.pdf"

# Extract sentences
sentences = extract_sentences_from_pdf(pdf_path)

for sentence in sentences:
    print(sentence)

"""**Sentence Embeddings**"""

!pip install sentence_transformers

from sentence_transformers import SentenceTransformer, util

# Load pre-trained model
model_sentence = SentenceTransformer('all-mpnet-base-v2')

"""**Vector Database Integration**"""

# list to store the sentence embeddings
vectors = []
for sentence in sentences:
  sentence_embedding = model_sentence.encode(sentence)
  vectors.append(sentence_embedding)

#query="How much funding did 'Facebook' receive from 'Accel Partners' in 2005?"
#query_embedding=model_sentence.encode(query)
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Function to find the nearest neighbors
def find_nearest_neighbors(query_embedding, sentence_embeddings, top_k=2):

    # Calculate cosine similarities
    similarities = cosine_similarity(query_embedding.reshape(1, -1), sentence_embeddings)

    # Get the top_k indices sorted by similarity (highest first)
    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]

    return top_k_indices

#nearest_neighbor_indices = find_nearest_neighbors(query_embedding,vectors, top_k=1)

#print("Indices of nearest neighbors:", nearest_neighbor_indices)

#for i in nearest_neighbor_indices:
  #print(sentences[i])

"""**Chatbot Framework**"""

import re


# fuction to join query and the additional info
def provide_answer(query, additional_info):
    for info in additional_info:
        query += " " + str(info)
    return query

def main():
    print("\033[1;31mWelcome to the Chatbot console!\033[0m")
    print("You can start chatting. Type 'exit' to quit.")

    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            print("Exiting...")
            break

        query_embedding = model_sentence.encode(user_input)
        nearest_neighbor_indices = find_nearest_neighbors(query_embedding, vectors, top_k=2)
        additional_info = ["according to my knowledge "] + [sentences[i] for i in nearest_neighbor_indices]
        #print(additional_info)

        full_query = provide_answer(user_input, additional_info)
        response_prompt = model.generate_content(full_query.replace("\n", " "))
        response_without_prompt = model.generate_content(user_input)

        print("Gemini_withprompt:", response_prompt.text)
        print("------------------------------------------------------------")
        print("Gemini_withoutprompt:", response_without_prompt.text)

if __name__ == "__main__":
    main()